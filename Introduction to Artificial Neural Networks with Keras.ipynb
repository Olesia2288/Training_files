{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Персептрон"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Персептрон — одна из простейших архитектур ИНС, изобретенная в 1957 году Фрэнком Розенблаттом .Он основан на несколько ином искусственном нейроне (см. рис. 10-4 ), называемом пороговым логическим блоком (TLU), а иногда и линейным пороговым блоком (LTU).Входные и выходные данные представляют собой числа (вместо двоичных значений включения/выключения), и каждому входному соединению присвоен вес. TLU сначала вычисляет линейную функцию своих входов: z = w 1 x 1 + w 2 x 2 + ⋯ + w n x n + b = w ⊺ x + b . Затем к результату применяется ступенчатая функция : h w ( x ) = Step( z ). Таким образом, это почти похоже на логистическую регрессию, за исключением того, что вместо логистической функции используется ступенчатая функция (Глава 4 ). Как и в логистической регрессии, параметрами модели являются входные веса w и член смещения b ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn предоставляет Perceptron класс, который можно использовать практически так, как и следовало ожидать, например, для набора данных ирисов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 0)  \n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "X_new = [[2, 0.5], [3, 1]]\n",
    "y_pred = per_clf.predict(X_new)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "алгоритм обучения перцептрона сильно напоминает стохастический градиентный спуск. Фактически, Perceptron класс Scikit-Learn эквивалентен использованию SGDClassifier со следующими гиперпараметрами: loss=\"perceptron\", learning_rate=\"constant\", eta0=1(скорость обучения) и penalty=None(без регуляризации )."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессионные MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-первых, MLP можно использовать для задач регрессии.Если вы хотите спрогнозировать одно значение (например, цену дома с учетом многих его характеристик), то вам нужен всего лишь один выходной нейрон: его выходом будет прогнозируемое значение. Для многомерной регрессии (т. е. для прогнозирования нескольких значений одновременно) вам нужен один выходной нейрон на каждое выходное измерение. Например, чтобы найти центр объекта на изображении, вам необходимо спрогнозировать двумерные координаты, поэтому вам нужны два выходных нейрона. Если вы также хотите разместить вокруг объекта ограничивающую рамку, вам понадобятся еще два числа: ширина и высота объекта. Итак, у вас есть четыре выходных нейрона."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Scikit-Learn есть MLPRegressorкласс, поэтому давайте воспользуемся им для создания MLP с тремя скрытыми слоями, состоящими из 50 нейронов каждый, и обучим его на наборе данных о жилье в Калифорнии."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код начинается с выборки и разделения набора данных, затем он создает конвейер для стандартизации входных объектов перед их отправкой в ​​файл MLPRegressor. Это очень важно для нейронных сетей, поскольку они обучаются с помощью градиентного спуска, Градиентный спуск не очень хорошо сходится, когда объекты имеют очень разные масштабы. \n",
    "\n",
    "Наконец, код обучает модель и оценивает ее ошибку проверки. Модель использует функцию активации ReLU в скрытых слоях и вариант градиентного спуска под названием Adam, чтобы минимизировать среднеквадратическую ошибку, с небольшой регуляризацией ℓ 2 (которой вы можете управлять с помощью alpha гиперпараметра):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing() # набор данных\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42) # три скрытых слоя по 50 нейронов\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_reg) # создаем контейнер с маштабированием и самой моделью\n",
    "pipeline.fit(X_train, y_train) # обучение\n",
    "y_pred = pipeline.predict(X_valid) # предсказание\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053326657968666"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получаем RMSE проверки около 0,505, что сопоставимо с тем, что вы получили бы с помощью классификатора случайного леса."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типичная архитектура регрессионного MLP:\n",
    "\n",
    "Гиперпараметр\t                           \n",
    "- скрытых слоев:               Зависит от проблемы, но обычно от 1 до 5.\n",
    "\n",
    "- нейронов на скрытый слой:    Зависит от проблемы, но обычно от 10 до 100.\n",
    "\n",
    "- выходных нейронов:           1 на каждое измерение прогноза\n",
    "\n",
    "- Скрытая активация: РеЛУ\n",
    "\n",
    "- Активация выхода: Нет, или ReLU/softplus (если выходные данные положительные) или сигмовидная/tanh (если выходные данные ограничены)\n",
    "\n",
    "- Функция потерь: MSE или Huber, если выбросы"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация МЛП"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP также можно использовать для задач классификации.Для задачи двоичной классификации вам нужен всего лишь один выходной нейрон с использованием сигмовидной функции активации: выходным сигналом будет число от 0 до 1, которое вы можете интерпретировать как предполагаемую вероятность положительного класса. Предполагаемая вероятность отрицательного класса равна единице минус это число."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP также могут легко решать задачи двоичной классификации по нескольким меткам. Например, у вас может быть система классификации электронной почты, которая предсказывает, является ли каждое входящее письмо спамом или нежелательным, и одновременно прогнозирует, является ли оно срочным или несрочным. В этом случае вам понадобятся два выходных нейрона, оба используют сигмовидную функцию активации: первый будет выводить вероятность того, что электронное письмо является спамом, а второй — вероятность того, что оно срочное. \n",
    "\n",
    "Если каждый экземпляр может принадлежать только одному классу из трех или более возможных классов (например, классы от 0 до 9 для классификации изображений цифр), то вам необходимо иметь один выходной нейрон для каждого класса, и вам следует использовать функцию активации softmax. для всего выходного слоя.Функция softmax гарантирует, что все оцененные вероятности находятся в диапазоне от 0 до 1 и что их сумма равна 1, поскольку классы являются исключительными. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типичная классификационная архитектура MLP\n",
    "\n",
    "Гиперпараметр\t\n",
    "- скрытых слоев: Обычно от 1 до 5 слоев, в зависимости от задачи.\n",
    "\n",
    "- выходных нейронов: 1, 1 на двоичную метку, 1 на класс\n",
    "\n",
    "- Активация выходного слоя: сигмовидная (Бинарная классификация и многозначная бинарная классификация), Софтмакс (Мультиклассовая классификация)\n",
    "\n",
    "- Функция потерь: X-энтропия"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP с Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
